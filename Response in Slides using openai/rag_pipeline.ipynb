{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates a pipeline for automatically generating LaTeX Beamer presentations based on a user's query. The process involves extracting relevant content from a PDF, embedding the text chunks, retrieving the most relevant sections using a search model, and generating LaTeX code with OpenAI's GPT model. Finally, the LaTeX code is compiled into a PDF presentation. This approach integrates document processing, natural language understanding, and LaTeX formatting to produce dynamic educational content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Necessary Libraries\n",
    "\n",
    "In this cell, we import all the required libraries for the task. The libraries include:\n",
    "\n",
    "- `openai`: Used to interact with OpenAI's API to generate responses for LaTeX Beamer code based on the provided query.\n",
    "- `my_secrets`: A custom module where sensitive information, like API keys, is stored securely.\n",
    "- `os`: Provides functions for interacting with the operating system, such as creating directories and running system commands.\n",
    "- `subprocess`: Used to run external system commands, in this case, to compile LaTeX code into a PDF.\n",
    "- `sentence_transformers`: A library for generating sentence embeddings (numerical representations) of text, which is used for comparing and retrieving chunks of text.\n",
    "- `faiss`: A library for efficient similarity search and clustering of dense vectors, which helps in finding the most relevant chunks of text based on a query.\n",
    "- `numpy`: A library used for handling arrays, which is essential for working with the embeddings.\n",
    "- `chunks`: A custom module that presumably contains the `MyChunks` class or object that holds the chunked text from the PDF.\n",
    "\n",
    "These libraries together provide the tools needed to process and search through large text documents (like PDFs), interact with OpenAI's API, and generate LaTeX Beamer presentations based on user queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import openai  # type: ignore\n",
    "import my_secrets\n",
    "import os\n",
    "import subprocess\n",
    "from sentence_transformers import SentenceTransformer  # type: ignore\n",
    "import faiss  # type: ignore\n",
    "import numpy as np  # type: ignore\n",
    "import chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Sentence Embeddings, FAISS Index, and Setting Up Directories\n",
    "\n",
    "In this cell, we perform several key initializations for embedding and indexing text chunks from a PDF document:\n",
    "\n",
    "1. **Loading Sentence Transformer Model**:\n",
    "\n",
    "   - We use the `SentenceTransformer` model `all-MiniLM-L6-v2` to generate sentence embeddings. This model is pre-trained to create dense vector representations (embeddings) of text, which can be used for tasks like similarity search and clustering.\n",
    "\n",
    "2. **Setting Embedding Dimension**:\n",
    "\n",
    "   - The `dimension` is set to 384, which corresponds to the number of features in the embeddings generated by the `all-MiniLM-L6-v2` model. This is the fixed size of the vector representations.\n",
    "\n",
    "3. **Creating FAISS Index**:\n",
    "   - FAISS (Facebook AI Similarity Search) is used to efficiently search through the embeddings. We initialize a `IndexFlatL2` index, which will store the embeddings in a way that allows us to quickly compute similarity searches (in this case, using L2 distance).\n",
    "4. **Setting Up Embedding Storage**:\n",
    "\n",
    "   - An empty list `embeddings` is created to store the embeddings generated for each chunk of text. This list will be used later to hold the embeddings of the text chunks from the PDF.\n",
    "\n",
    "5. **Creating Directory for Extracted Images**:\n",
    "\n",
    "   - We define a directory called `extracted_images` to store any images that might be extracted from the PDF. The `os.makedirs` function ensures that this directory is created if it doesn't already exist.\n",
    "\n",
    "6. **Initializing Chunks**:\n",
    "   - The `MyChunks` object is assumed to be a collection of text chunks from the PDF, which will be used later to generate embeddings and perform similarity searches.\n",
    "\n",
    "This setup is essential for embedding the text chunks from the PDF, storing them in an index, and making them ready for querying based on the user's input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "dimension = 384\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "embeddings = []\n",
    "image_dir = \"extracted_images\"\n",
    "os.makedirs(image_dir, exist_ok=True)\n",
    "MyChunks = chunks.Mychunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding and Indexing Chunks\n",
    "\n",
    "In this cell, the function `embed_and_index_chunks()` is responsible for embedding the text chunks from the PDF and indexing them for efficient retrieval. Here's a breakdown of what this function does:\n",
    "\n",
    "1. **Check if Chunks Are Already Embedded**:\n",
    "\n",
    "   - The function first checks if the `embeddings` list is already populated. If the embeddings are already computed, the function prints `\"Chunks already embedded and indexed.\"` and exits early, avoiding redundant work.\n",
    "\n",
    "2. **Embedding Text from Chunks**:\n",
    "\n",
    "   - If the embeddings are not yet created, the function loops through each chunk in the `MyChunks` collection.\n",
    "   - For each chunk, it extracts the `text` field and passes it to the `embedding_model.encode()` method, which generates an embedding (a vector representation) of the text. This embedding is then appended to the `embeddings` list.\n",
    "\n",
    "3. **Indexing the Embeddings**:\n",
    "   - Each embedding is added to the FAISS index using the `index.add()` method. Since FAISS requires the embeddings to be in the form of `float32` arrays, the code converts the embedding to the appropriate format using `np.array([embedding]).astype(\"float32\")`.\n",
    "\n",
    "The purpose of this function is to convert the text chunks into numerical representations (embeddings), which can then be efficiently searched using FAISS. This process prepares the data for quick retrieval based on a query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_and_index_chunks():\n",
    "    global embeddings\n",
    "    if embeddings:\n",
    "        print(\"Chunks already embedded and indexed.\")\n",
    "        return\n",
    "    for chunk in MyChunks:\n",
    "        text = chunk['text']\n",
    "        embedding = embedding_model.encode(text)\n",
    "        embeddings.append(embedding)\n",
    "        index.add(np.array([embedding]).astype(\"float32\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving Relevant Chunks Based on Query\n",
    "\n",
    "In this cell, the function `retrieve_chunks(query, top_k=10)` is designed to retrieve the most relevant chunks from the indexed data based on a user's query. Here's how it works:\n",
    "\n",
    "1. **Generating Query Embedding**:\n",
    "\n",
    "   - The input `query` (a string) is passed to the `embedding_model.encode()` method to generate an embedding (numerical representation) for the query.\n",
    "\n",
    "2. **Searching for Closest Chunks**:\n",
    "\n",
    "   - The generated query embedding is then passed to the FAISS index using the `index.search()` method. This performs a search to find the nearest vectors (embeddings) to the query embedding.\n",
    "   - The `top_k` parameter determines how many closest results (chunks) will be retrieved. By default, it retrieves the top 10 most relevant chunks.\n",
    "\n",
    "3. **Returning Relevant Chunks**:\n",
    "   - The `indices` returned by FAISS are used to select the corresponding chunks from the `MyChunks` list. The function returns these top `k` chunks that are most similar to the query.\n",
    "\n",
    "This function enables the system to retrieve the most relevant text chunks based on the content of a user's query by comparing the query's embedding with the precomputed embeddings in the FAISS index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_chunks(query, top_k=10):\n",
    "    query_embedding = embedding_model.encode(query)\n",
    "    distances, indices = index.search(\n",
    "        np.array([query_embedding]).astype(\"float32\"), top_k)\n",
    "    return [MyChunks[i] for i in indices[0]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Answer Based on Retrieved Chunks\n",
    "\n",
    "In this cell, the function `generate_answer(query, retrieved_chunks)` is responsible for generating a structured prompt that can be used to answer a user's question based on the content of the retrieved chunks. Here's how it works:\n",
    "\n",
    "1. **Creating the Context**:\n",
    "\n",
    "   - The function begins by creating a context string that consists of the relevant chunks retrieved from the previous step. Each chunk is formatted to display the page number and the corresponding text.\n",
    "   - The chunks are joined together with two newlines (`\\n\\n`) for better readability.\n",
    "\n",
    "2. **Answer Generation Prompt**:\n",
    "   - The function constructs a final prompt that includes:\n",
    "     - The context made up of relevant chunks.\n",
    "     - The user's query, which is passed as the input `query` parameter.\n",
    "   - This prompt is designed to instruct a language model (e.g., OpenAI) to generate an answer based on the provided context.\n",
    "\n",
    "The generated prompt is returned as a structured string, which will be used in subsequent steps to call a language model (like OpenAI's GPT) to generate an answer based on the content of the textbook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query, retrieved_chunks):\n",
    "    context = \"\\n\\n\".join(\n",
    "        [f\"Page {chunk['page']}: {chunk['text']}\" for chunk in retrieved_chunks])\n",
    "    return f\"Answer the question based on the following textbook content:\\n\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the RAG Pipeline\n",
    "\n",
    "In this cell, we define the function `run_rag_pipeline(pdf_path, query)` to execute the full process of retrieving relevant content from the PDF and generating an answer to a user's query. The function performs the following steps:\n",
    "\n",
    "1. **Check for Available Chunks**:\n",
    "\n",
    "   - First, the function checks if there are any chunks in `MyChunks`. If there are no chunks, it returns the message `\"No data available.\"`. This ensures that the pipeline doesn't run unless there is data to process.\n",
    "\n",
    "2. **Embedding and Indexing**:\n",
    "\n",
    "   - If chunks are available, it calls `embed_and_index_chunks()` to generate embeddings for each chunk and index them using FAISS. This prepares the chunks for efficient retrieval.\n",
    "\n",
    "3. **Retrieve Relevant Chunks**:\n",
    "\n",
    "   - It then calls `retrieve_chunks(query)` to fetch the most relevant chunks based on the user's query. The function uses the embeddings and FAISS indexing to perform this retrieval efficiently.\n",
    "\n",
    "4. **Generate Answer**:\n",
    "   - Finally, it calls `generate_answer(query, retrieved_chunks)` to generate a detailed response to the user's query, using the relevant chunks as context.\n",
    "\n",
    "The final output of the function is the generated answer based on the queried content from the PDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rag_pipeline(pdf_path, query):\n",
    "    if not MyChunks:\n",
    "        return \"No data available.\"\n",
    "    embed_and_index_chunks()\n",
    "    retrieved_chunks = retrieve_chunks(query)\n",
    "    return generate_answer(query, retrieved_chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the OpenAI API Key\n",
    "\n",
    "In this cell, we define the function `load_openai_key()` to load the API key for interacting with the OpenAI API.\n",
    "\n",
    "- The function retrieves the API key from the `my_secrets` module, where it is securely stored in the variable `OPEN_AI_SECRET_KEY`. This key is necessary for making requests to the OpenAI API.\n",
    "\n",
    "By keeping the key in a separate module like `my_secrets`, we can ensure that sensitive information like API keys is not hardcoded into the main code and is more secure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_openai_key():\n",
    "    return my_secrets.OPEN_AI_SECRET_KEY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling OpenAI's Chat API for LaTeX Beamer Code Generation\n",
    "\n",
    "In this cell, we define the function `call_openai_chat()`, which interacts with the OpenAI API to generate LaTeX Beamer code based on the provided prompt.\n",
    "\n",
    "- **Input**: The function takes two parameters:\n",
    "  - `prompt`: A string containing the userâ€™s prompt that describes what kind of LaTeX Beamer code is required.\n",
    "  - `model`: The model to use for generating the output, which defaults to \"gpt-3.5-turbo\".\n",
    "- **Process**:\n",
    "\n",
    "  - The function sets the OpenAI API key using the `load_openai_key()` function defined earlier.\n",
    "  - It then sends the `prompt` to the API, along with a system message indicating the assistant is an expert in LaTeX and Beamer presentations.\n",
    "  - The response from the API is expected to contain LaTeX Beamer code, which is then returned after stripping any leading/trailing whitespace.\n",
    "\n",
    "- **Error Handling**: The function includes a try-except block to catch any errors during the API call and return an appropriate error message.\n",
    "\n",
    "This function is essential for automatically generating LaTeX Beamer code that can later be compiled into a presentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_openai_chat(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    openai.api_key = load_openai_key()\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"system\", \"content\": \"You are an expert in LaTeX and Beamer presentations.\"},\n",
    "                      {\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=4000,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return response['choices'][0]['message']['content'].strip()\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving LaTeX Code to a File\n",
    "\n",
    "This cell defines the function `save_to_file()`, which is responsible for saving the generated LaTeX code to a `.tex` file.\n",
    "\n",
    "- **Input**:\n",
    "\n",
    "  - `filename`: The name of the file where the LaTeX code will be saved.\n",
    "  - `content`: The LaTeX code (as a string) that needs to be written to the file.\n",
    "\n",
    "- **Process**:\n",
    "\n",
    "  - The function opens the specified `filename` in write mode (`'w'`).\n",
    "  - It then writes the `content` (LaTeX code) into the file. If the file doesn't already exist, it will be created.\n",
    "\n",
    "- **Purpose**: This function is used to save the LaTeX Beamer code generated by the OpenAI API so that it can later be compiled into a PDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_file(filename, content):\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling LaTeX Code into a PDF\n",
    "\n",
    "This cell defines the function `compile_latex_to_pdf()`, which is responsible for compiling the saved LaTeX `.tex` file into a PDF using `pdflatex`.\n",
    "\n",
    "- **Input**:\n",
    "\n",
    "  - `tex_file`: The name of the `.tex` file that needs to be compiled.\n",
    "\n",
    "- **Process**:\n",
    "\n",
    "  - The function attempts to run the `pdflatex` command on the provided `.tex` file using the `subprocess.run()` function.\n",
    "  - If the compilation is successful, it checks whether the resulting `.pdf` file exists and returns the path to the generated PDF.\n",
    "  - If the compilation fails (e.g., due to syntax errors in the LaTeX code), the function catches the error and prints an error message.\n",
    "\n",
    "- **Purpose**: This function is used to convert the LaTeX Beamer code into a viewable PDF document, making the presentation ready for display or further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_latex_to_pdf(tex_file):\n",
    "    try:\n",
    "        subprocess.run([\"pdflatex\", tex_file], check=True)\n",
    "        pdf_file = tex_file.replace('.tex', '.pdf')\n",
    "        return pdf_file if os.path.exists(pdf_file) else None\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error compiling LaTeX: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Valid LaTeX Code from Raw Response\n",
    "\n",
    "This cell defines the function `extract_latex_code()`, which processes a raw response from the OpenAI API to extract only the valid LaTeX code, filtering out unnecessary parts such as code block markers.\n",
    "\n",
    "- **Input**:\n",
    "\n",
    "  - `raw_response`: The raw text response returned by the OpenAI API. This response may contain code block markers (i.e., ` ```latex ` and ` ``` `), which need to be filtered out.\n",
    "\n",
    "- **Process**:\n",
    "\n",
    "  - The function iterates through the lines of the response.\n",
    "  - It identifies the lines that start with ` ```latex ` to mark the beginning of LaTeX code.\n",
    "  - It then collects the LaTeX code lines until it reaches the closing code block marker ` ``` `.\n",
    "  - These valid LaTeX code lines are stored in a list (`cleaned_lines`), and the final LaTeX code is returned as a single string.\n",
    "\n",
    "- **Purpose**: This function ensures that only the LaTeX code (within the designated code block) is returned, without any extra formatting or metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_latex_code(raw_response):\n",
    "    lines = raw_response.split(\"\\n\")\n",
    "    in_code_block = False\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        if line.strip().startswith(\"```latex\"):\n",
    "            in_code_block = True\n",
    "            continue\n",
    "        if line.strip() == \"```\":\n",
    "            in_code_block = False\n",
    "            continue\n",
    "        if in_code_block:\n",
    "            cleaned_lines.append(line)\n",
    "    return \"\\n\".join(cleaned_lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Execution: Get User's Prompt, Process the Document, Generate LaTeX Beamer Code, and Compile into PDF\n",
    "\n",
    "This cell runs the complete process starting from getting the user's query, retrieving relevant chunks of text, generating LaTeX Beamer code, and compiling it into a PDF.\n",
    "\n",
    "- **Step 1: Get the user's query**\n",
    "\n",
    "  - The script prompts the user for input, asking them to enter their question or query.\n",
    "\n",
    "- **Step 2: Retrieve relevant chunks from the textbook**\n",
    "\n",
    "  - The `run_rag_pipeline()` function processes the provided PDF document and retrieves relevant text chunks based on the user's query.\n",
    "\n",
    "- **Step 3: Generate LaTeX Beamer code using OpenAI**\n",
    "\n",
    "  - A prompt is formulated to request OpenAI to generate a detailed LaTeX Beamer presentation on the topic, including equations, bullet points, and TikZ-based diagrams.\n",
    "  - The `call_openai_chat()` function is used to interact with the OpenAI API and generate the LaTeX code.\n",
    "\n",
    "- **Step 4: Extract valid LaTeX code**\n",
    "\n",
    "  - The raw response from OpenAI is processed using `extract_latex_code()` to clean and retrieve only the valid LaTeX code.\n",
    "\n",
    "- **Step 5: Save LaTeX code to a .tex file**\n",
    "\n",
    "  - The extracted LaTeX code is saved to a `.tex` file using `save_to_file()`.\n",
    "\n",
    "- **Step 6: Compile the .tex file into a PDF**\n",
    "  - The `.tex` file is compiled into a PDF using the `compile_latex_to_pdf()` function.\n",
    "  - If successful, the generated PDF filename is printed; otherwise, a failure message is shown.\n",
    "\n",
    "This cell encapsulates the entire RAG pipeline and the process of transforming a user's query into a LaTeX Beamer presentation ready for compilation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF generated: response.pdf\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Main execution - Get user's prompt, process the document, generate LaTeX Beamer code, and compile into PDF\n",
    "\n",
    "# Step 1: Get the user's query\n",
    "user_prompt = input(\"Enter your prompt: \")\n",
    "\n",
    "# Step 2: Retrieve relevant chunks from the textbook\n",
    "pdf_path = \"Physics 9.pdf\"\n",
    "answer = run_rag_pipeline(pdf_path, user_prompt)\n",
    "\n",
    "# Step 3: Generate LaTeX Beamer code using OpenAI\n",
    "beamer_prompt = (\n",
    "    f\"Create a detailed LaTeX Beamer presentation on the following topic:\\n\\n\"\n",
    "    f\"{answer}\\n\\n\"\n",
    "    \"Include equations, bullet points, and TikZ-based diagrams to illustrate concepts. Use only TikZ to draw shapes or vectors \"\n",
    "    \"instead of relying on external image files. Ensure the output is ready-to-compile Beamer code.\"\n",
    ")\n",
    "raw_beamer_code = call_openai_chat(beamer_prompt)\n",
    "\n",
    "# Step 4: Extract valid LaTeX code\n",
    "beamer_code = extract_latex_code(raw_beamer_code)\n",
    "\n",
    "# Step 5: Save LaTeX code to a .tex file\n",
    "tex_filename = \"response.tex\"\n",
    "save_to_file(tex_filename, beamer_code)\n",
    "\n",
    "# Step 6: Compile the .tex file into a PDF\n",
    "pdf_filename = compile_latex_to_pdf(tex_filename)\n",
    "if pdf_filename:\n",
    "    print(f\"PDF generated: {pdf_filename}\")\n",
    "else:\n",
    "    print(\"Failed to generate PDF.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
